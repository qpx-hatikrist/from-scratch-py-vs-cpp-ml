# Comparison of Machine Learning Models: Custom Implementations vs scikit-learn (C++ and Python)

[üá¨üáß English](README.md) | [üá∑üá∫ –†—É—Å—Å–∫–∏–π](README_ru.md)

## EN :uk: / RU :ru:

**EN :uk:**

A project for experimental comparison of classic machine learning models implemented in three variants:

1. **`scikit-learn` library** (baseline);
2. **Custom implementation in Python**;
3. **Custom implementation in C++**.

The comparison is based on two main criteria:

- **model quality** (error / accuracy on the test data);
- **performance** (training and inference time).

At the moment, **linear regression**, **Ridge**, **Lasso** is implemented and compared; other models are under development.

---

## Project goals

- Gain practical understanding of how close one can get to `scikit-learn` by implementing models ‚Äúfrom scratch‚Äù.
- Explore the difference between Python and C++ when implementing the same algorithms:
  - language and runtime overhead;
  - development convenience;
  - code maintainability.
- Build reproducible experiments that allow you to:
  - run identical training and testing scenarios;
  - log quality metrics;
  - perform fair time benchmarks.

---

## Repository architecture and structure

```text
from-scratch-py-vs-cpp-ml/
‚îú‚îÄ‚îÄ data/                 # Source data for experiments
‚îÇ   ‚îî‚îÄ‚îÄ regression/       # Datasets for regression tasks
‚îú‚îÄ‚îÄ notebooks/            # Jupyter notebooks with experiments and visualizations
‚îú‚îÄ‚îÄ src/                  # Algorithm implementations in C++ and Python
‚îú‚îÄ‚îÄ vendor/               # External / helper code (headers, utilities, etc.)
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îî‚îÄ‚îÄ .gitmodules           # Git submodules configuration
```

---

### Metrics and preprocessing implementation

Within the project we **deliberately implement by hand** both the main metrics and data preprocessing:

**Metrics (Python and C++):**

- R¬≤  
- D¬≤  
- RMSE  
- SMAPE  
- RMSLE  
- MAXE
- MedAE
- EVS

**Preprocessing:**

- `StandardScaler` ‚Äî custom implementation of feature standardization (estimating mean and standard deviation on the train part and applying it to train/test);
- `train_test_split` ‚Äî custom implementation of splitting the dataset into train / test with a fixed random seed and shuffle.

The goals of this approach are:

- better understand numerical behavior and possible sources of errors;
- compare not only the models, but also the cost of computing metrics / transformations in Python and C++;

At the same time, **we intentionally use `scikit-learn` only as an analysis utility**:

- **`learning_curve`**  
- **`permutation_test_score`**

These functions are taken from `sklearn.model_selection` and used for:

- building learning curves and analyzing generalization ability;
- assessing the statistical significance of model quality.

All experimental notebooks explicitly indicate which parts of the pipeline are implemented manually and which parts use `scikit-learn` as a helper tool.

---

### Linear regression

Evaluation is performed **on the same dataset** (for now we use a single dataset; later we plan to add several heterogeneous datasets) using the following set of metrics:

- **R¬≤** ‚Äî coefficient of determination;
- **D¬≤** ‚Äî a modification of R¬≤ (deviance-based score) for assessing prediction quality;
- **RMSE** ‚Äî a variation of mean squared error (mean root square error);
- **SMAPE** ‚Äî symmetric mean absolute percentage error;
- **RMSLE** ‚Äî root mean squared logarithmic error;
- **MAXE** ‚Äî maximum absolute error (the worst mistake the model makes on a single instance);
- **MedAE** ‚Äî median absolute error (a typical error, robust to outliers);
- **EVS** ‚Äî proportion of the target variable‚Äôs variance explained by the model (how well the model explains the target‚Äôs variation).

In addition to point metrics, we use tools to analyze generalization and statistical significance:

- **`learning_curve`** ‚Äî to build learning curves and analyze how quality depends on the size of the training set;
- **`permutation_test_score`** ‚Äî to assess the statistical significance of model quality and check whether the result could be due to chance.

| Stack          | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 21461.048 | 0.928407 | 0.7396   | 158451.5957 | 10961.8721  | 0.92844  | 0.078459     | 0.000679      |
| Python scratch | 21461.048 | 0.928407 | 0.7396   | 158451.5951 | 10961.8719  | 0.92844  | 39.34056     | 0.050775      |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**Conclusion (LinearRegression):** `ScratchLinearRegression` matches `sklearn.LinearRegression` in terms of quality (differences are at machine precision level), which means the implementation is mathematically correct. Training time is ~**√ó500** slower and prediction time is ~**√ó70** slower due to pure Python loops.

*A more detailed analysis (learning curves, permutation test, metric comparison and implementation details) is available in the notebook [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*

---

### Ridge

| Impl           | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 21162.30  | 0.930386 | 0.744151 | 158372.79   | 10550.75    | 0.930414 | 0.025        | 0.002         |
| Python scratch | 21162.30  | 0.930386 | 0.744151 | 158372.79   | 10550.75    | 0.930414 | 38.08        | 0.051         |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**Conclusion (Ridge):** my implementation exactly matches `sklearn.Ridge` across all metrics (differences are within machine precision), but pure Python training is ~**√ó1500** slower and inference is ~**√ó25** slower. Mathematically it is correct, but in terms of speed it is an educational/reference implementation.

*A more detailed analysis (learning curves, permutation test, metric and implementation comparison) is provided in the notebook [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*

---

### Lasso

| Impl           | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 20929.67  | 0.931908 | 0.752938 | 160524.52   | 10362.82    | 0.931929 | 0.089        | 0.00093       |
| Python scratch | 20928.15  | 0.931918 | 0.752961 | 160520.72   | 10361.59    | 0.931939 | 226.95       | 0.10144       |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**Conclusion (Lasso):** `ScratchLasso` produces almost the same RMSE, R¬≤, D¬≤, EVS, MAXE and MedAE as `sklearn.Lasso` (differences at the 3rd‚Äì4th decimal place), which means the implementation is correct. The cost is time: training is ~**√ó2500** slower and prediction is ~**√ó100** slower.

*A more detailed analysis (learning curves, permutation test, metric and implementation comparison) is provided in the notebook [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*


---

## **RU :ru:**

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: —Å–≤–æ–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ vs scikit-learn (C++ –∏ Python)

–ü—Ä–æ–µ–∫—Ç –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ —Ç—Ä—ë—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:

1. **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `scikit-learn`** (baseline);
2. **–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ Python**;
3. **–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ C++**.

–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ–¥—ë—Ç—Å—è –ø–æ –¥–≤—É–º –æ—Å–Ω–æ–≤–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:

- **–∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏** (–æ—à–∏–±–∫–∞ / —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö);
- **–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** (–≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è).

–í –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è **–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è**, **Ridge**, **Lasso**; –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ.

---

## –¶–µ–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞

- –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–æ –∫ `scikit-learn` –º–æ–∂–Ω–æ –ø–æ–¥–æ–π—Ç–∏, —Ä–µ–∞–ª–∏–∑—É—è –º–æ–¥–µ–ª–∏ ¬´–≤—Ä—É—á–Ω—É—é¬ª.
- –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É Python –∏ C++ –ø—Ä–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤:
  - –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã —è–∑—ã–∫–∞ –∏ —Ä–∞–Ω—Ç–∞–π–º–∞;
  - —É–¥–æ–±—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏;
  - —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–æ–¥–∞.
- –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ:
  - –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è;
  - —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞;
  - –ø—Ä–æ–≤–æ–¥–∏—Ç—å —á–µ—Å—Ç–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏.

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```text
from-scratch-py-vs-cpp-ml/
‚îú‚îÄ‚îÄ data/                 # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ regression/       # –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
‚îú‚îÄ‚îÄ notebooks/            # Jupyter-–Ω–æ—É—Ç–±—É–∫–∏ —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
‚îú‚îÄ‚îÄ src/                  # –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ C++ –∏ Python
‚îú‚îÄ‚îÄ vendor/               # –í–Ω–µ—à–Ω–∏–π/–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–¥ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —É—Ç–∏–ª–∏—Ç—ã –∏ –ø—Ä.)
‚îú‚îÄ‚îÄ requirements.txt      # Python-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ .gitmodules           # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è git-–ø–æ–¥–º–æ–¥—É–ª–µ–π
```

---

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏

–í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –º—ã **–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ —Ä–µ–∞–ª–∏–∑—É–µ–º –≤—Ä—É—á–Ω—É—é** –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö:

**–ú–µ—Ç—Ä–∏–∫–∏ (Python –∏ C++):**

- R¬≤  
- D¬≤  
- RMSE  
- SMAPE  
- RMSLE
- MAXE
- MedAE
- EVS

**–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞:**

- `StandardScaler` ‚Äî —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ—Ü–µ–Ω–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø–æ train-—á–∞—Å—Ç–∏, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ train/test).
- `train_test_split` ‚Äî —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ train / test —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º random seed –∏ shuffle.


–¶–µ–ª–∏ —Ç–∞–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:

- –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å —á–∏—Å–ª–µ–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—à–∏–±–æ–∫;
- —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏, –Ω–æ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫/–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –≤ Python –∏ C++;

–ü—Ä–∏ —ç—Ç–æ–º –º—ã **—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º `scikit-learn` —Ç–æ–ª—å–∫–æ –∫–∞–∫ —É—Ç–∏–ª–∏—Ç—É –∞–Ω–∞–ª–∏–∑–∞**:

- **`learning_curve`**  
- **`permutation_test_score`**

–≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ `sklearn.model_selection` –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è:

- –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏;
- –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π.

–í–æ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –Ω–æ—É—Ç–±—É–∫–∞—Ö —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–æ, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤—Ä—É—á–Ω—É—é, –∞ –∫–∞–∫–∏–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `sklearn` –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.

---

### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è **–Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –¥–∞—Ç–∞—Å–µ—Ç–µ** (–ø–æ–∫–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç; –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –ø–ª–∞–Ω–∏—Ä—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö) –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –Ω–∞–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫:

- **R¬≤** ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏;
- **D¬≤** ‚Äî –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è R¬≤ (deviance-based score) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π;
- **RMSE** ‚Äî –≤–∞—Ä–∏–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–µ–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –æ—à–∏–±–∫–∏ (mean root square error);
- **MAXE** ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (–Ω–∞–∏—Ö—É–¥—à–∏–π –ø—Ä–æ–º–∞—Ö –º–æ–¥–µ–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É);
- **MedAE** ‚Äî –º–µ–¥–∏–∞–Ω–Ω–∞—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (—Ç–∏–ø–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞, —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º);
- **EVS** ‚Äî –¥–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –æ–±—ä—è—Å–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å –æ–±—ä—è—Å–Ω—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏—é —Ç–∞—Ä–≥–µ—Ç–∞).

–ü–æ–º–∏–º–æ —Ç–æ—á–µ—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:

- **`learning_curve`** ‚Äî –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏;
- **`permutation_test_score`** ‚Äî –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏.

| Stack          | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 21461.048 | 0.928407 | 0.739617 | 158451.5957 | 10961.87213 | 0.92844  | 0.0784       | 0.000679      |
| Python scratch | 21461.048 | 0.928407 | 0.739617 | 158451.5951 | 10961.87198 | 0.92844  | 39.340       | 0.050775      |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**–í—ã–≤–æ–¥ (LinearRegression):** `ScratchLinearRegression` –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å `sklearn.LinearRegression` (—Ä–∞–∑–ª–∏—á–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–∞—à–∏–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏), —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ **√ó500** —Ä–∞–∑ –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äî –≤ **√ó70** —Ä–∞–∑ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –∏–∑-–∑–∞ —Ü–∏–∫–ª–æ–≤ –Ω–∞ —á–∏—Å—Ç–æ–º Python.

*–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è, permutation test, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π) –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –Ω–æ—É—Ç–±—É–∫–µ [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*

---

### Ridge

| Impl           | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 21162.30  | 0.930386 | 0.744151 | 158372.79   | 10550.75    | 0.930414 | 0.025        | 0.002         |
| Python scratch | 21162.30  | 0.930386 | 0.744151 | 158372.79   | 10550.75    | 0.930414 | 38.08        | 0.051         |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**–í—ã–≤–æ–¥ (Ridge):** –º–æ—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å–æ `sklearn.Ridge` –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º (—Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –º–∞—à–∏–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏), –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –≤ —á–∏—Å—Ç–æ–º Python ~**√ó1500** –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ~**√ó25** –º–µ–¥–ª–µ–Ω–Ω–µ–µ. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞, –Ω–æ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —ç—Ç–æ —á–∏—Å—Ç–æ —É—á–µ–±–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è.

*–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è, permutation test, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π) –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –Ω–æ—É—Ç–±—É–∫–µ [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*

---

### Lasso

| Impl           | RMSE      | R¬≤       | D¬≤       | MAXE        | MedAE       | EVS      | fit_time (s) | pred_time (s) |
|----------------|----------:|---------:|---------:|------------:|------------:|---------:|-------------:|--------------:|
| sklearn        | 20929.67  | 0.931908 | 0.752938 | 160524.52   | 10362.82    | 0.931929 | 0.089        | 0.00093       |
| Python scratch | 20928.15  | 0.931918 | 0.752961 | 160520.72   | 10361.59    | 0.931939 | 226.95       | 0.10144       |
| C++ scratch    | *TBD*     | *TBD*    | *TBD*    | *TBD*       | *TBD*       | *TBD*    | *TBD*        | *TBD*         |

**–í—ã–≤–æ–¥ (Lasso):** `ScratchLasso` –¥–∞—ë—Ç –ø–æ—á—Ç–∏ —Ç–µ –∂–µ –∑–Ω–∞—á–µ–Ω–∏—è RMSE, R¬≤, D¬≤, EVS, MAXE –∏ MedAE, —á—Ç–æ –∏ `sklearn.Lasso` (—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –Ω–∞ 3‚Äì4 –∑–Ω–∞–∫), —Ç–æ –µ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –¶–µ–Ω–∞ ‚Äî –≤—Ä–µ–º—è: –æ–±—É—á–µ–Ω–∏–µ ~**√ó2500** –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ~**√ó100** –º–µ–¥–ª–µ–Ω–Ω–µ–µ.

*–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è, permutation test, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π) –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –Ω–æ—É—Ç–±—É–∫–µ [`notebooks/linear_regression.ipynb`](notebooks/linear_regression.ipynb).*